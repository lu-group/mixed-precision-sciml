{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "yunIeisLMr3f",
   "metadata": {
    "id": "yunIeisLMr3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DDE_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env DDE_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3294611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3294611",
    "outputId": "8a98983e-27f1-4e10-dadb-62ace2c92759"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 17:11:25.528857: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 17:11:25.556619: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-26 17:11:25.556645: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-26 17:11:25.556665: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-26 17:11:25.561510: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 17:11:25.561937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 17:11:26.275107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using backend: tensorflow\n",
      "Other supported backends: tensorflow.compat.v1, pytorch, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the default float type to float32\n",
      "Compiling model...\n",
      "'compile' took 0.002501 s\n",
      "\n",
      "Training model...\n",
      "\n",
      "Step      Train loss    Test loss     Test metric   \n",
      "0         [2.57e-01]    [2.19e-01]    [1.02e+00]    \n",
      "\n",
      "Best model at step 0:\n",
      "  train loss: 2.57e-01\n",
      "  test loss: 2.19e-01\n",
      "  test metric: [1.02e+00]\n",
      "\n",
      "'train' took 0.338598 s\n",
      "\n",
      "Set the default float type to float16\n",
      "Compiling model...\n",
      "'compile' took 0.002478 s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/dev/lu-group/mixed-precision-sciml/.direnv/python-3.10/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "\n",
      "Step      Train loss    Test loss     Test metric   \n",
      "0         [4.05e-01]    [3.54e-01]    [1.30e+00]    \n",
      "\n",
      "Best model at step 0:\n",
      "  train loss: 4.05e-01\n",
      "  test loss: 3.54e-01\n",
      "  test metric: [1.30e+00]\n",
      "\n",
      "'train' took 0.070389 s\n",
      "\n",
      "{'_self_setattr_tracking': True, '_obj_reference_counts_dict': ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping True>: 1, <_ObjectIdentityWrapper wrapping <keras.src.saving.serialization_lib.Config object at 0x7f220fe51330>>: 1, <_ObjectIdentityWrapper wrapping 0>: 1, <_ObjectIdentityWrapper wrapping ListWrapper([<keras.src.layers.core.dense.Dense object at 0x7f220fc49480>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49510>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49810>])>: 1}), '_auto_get_config': True, '_auto_config': <keras.src.saving.serialization_lib.Config object at 0x7f220fe51330>, '_is_model_for_instrumentation': True, '_instrumented_keras_api': True, '_instrumented_keras_layer_class': False, '_instrumented_keras_model_class': True, '_trainable': True, '_stateful': False, 'built': True, '_input_spec': None, '_build_input_shape': TensorShape([16, 1]), '_saved_model_inputs_spec': TensorSpec(shape=(16, 1), dtype=tf.float32, name='input_1'), '_saved_model_arg_spec': ([TensorSpec(shape=(16, 1), dtype=tf.float32, name='input_1')], {}), '_supports_masking': False, '_name': 'fnn', '_activity_regularizer': None, '_trainable_weights': [], '_non_trainable_weights': [], '_updates': [], '_thread_local': <_thread._local object at 0x7f22134483b0>, '_callable_losses': [], '_losses': [], '_metrics': [], '_metrics_lock': <unlocked _thread.lock object at 0x7f220fc52ec0>, '_dtype_policy': <Policy \"float32\">, '_compute_dtype_object': tf.float32, '_autocast': True, '_self_tracked_trackables': [ListWrapper([<keras.src.layers.core.dense.Dense object at 0x7f220fc49480>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49510>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49810>])], '_inbound_nodes_value': [], '_outbound_nodes_value': [], '_call_spec': <keras.src.utils.layer_utils.CallFunctionSpec object at 0x7f220fc48d60>, '_dynamic': False, '_initial_weights': None, '_auto_track_sub_layers': True, '_preserve_input_structure_in_config': False, '_name_scope_on_declaration': '', '_captured_weight_regularizer': [], '_is_graph_network': False, 'inputs': None, 'outputs': None, 'input_names': None, 'output_names': None, 'stop_training': False, 'history': None, 'compiled_loss': None, 'compiled_metrics': None, '_compute_output_and_mask_jointly': False, '_is_compiled': False, 'optimizer': None, '_distribution_strategy': None, '_distribute_reduction_method': None, '_cluster_coordinator': None, '_run_eagerly': None, 'train_function': None, 'test_function': None, 'predict_function': None, 'train_tf_function': None, '_compiled_trainable_state': <WeakKeyDictionary at 0x7f220fc48f40>, '_training_state': None, '_self_unconditional_checkpoint_dependencies': [TrackableReference(name=denses, ref=ListWrapper([<keras.src.layers.core.dense.Dense object at 0x7f220fc49480>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49510>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49810>]))], '_self_unconditional_dependency_names': {'denses': ListWrapper([<keras.src.layers.core.dense.Dense object at 0x7f220fc49480>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49510>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49810>])}, '_self_unconditional_deferred_dependencies': {}, '_self_update_uid': -1, '_self_name_based_restores': set(), '_self_saveable_object_factories': {}, '_checkpoint': <tensorflow.python.checkpoint.checkpoint.Checkpoint object at 0x7f220fc48fd0>, '_steps_per_execution': None, '_enable_tune_steps_per_execution': False, '_layout_map': None, '_train_counter': <tf.Variable 'Variable:0' shape=() dtype=int64, numpy=0>, '_test_counter': <tf.Variable 'Variable:0' shape=() dtype=int64, numpy=0>, '_predict_counter': <tf.Variable 'Variable:0' shape=() dtype=int64, numpy=0>, '_base_model_initialized': True, '_jit_compile': None, '_input_transform': None, '_output_transform': None, 'regularizer': None, 'dropout_rate': 0, 'denses': ListWrapper([<keras.src.layers.core.dense.Dense object at 0x7f220fc49480>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49510>, <keras.src.layers.core.dense.Dense object at 0x7f220fc49810>]), '_auxiliary_vars': None}\n",
      "Set the default float type to float32\n",
      "Training model...\n",
      "\n",
      "Step      Train loss    Test loss     Test metric   \n",
      "0         [2.57e-01]    [2.19e-01]    [1.02e+00]    \n",
      "1000      [3.19e-02]    [3.39e-02]    [4.03e-01]    \n",
      "2000      [2.43e-04]    [2.43e-04]    [3.41e-02]    \n",
      "3000      [1.19e-04]    [1.49e-04]    [2.67e-02]    \n",
      "4000      [8.85e-05]    [1.25e-04]    [2.44e-02]    \n",
      "5000      [7.70e-05]    [1.13e-04]    [2.33e-02]    \n",
      "6000      [6.54e-05]    [1.00e-04]    [2.19e-02]    \n",
      "7000      [5.30e-05]    [8.62e-05]    [2.03e-02]    \n",
      "8000      [4.12e-05]    [7.27e-05]    [1.87e-02]    \n",
      "9000      [3.16e-05]    [6.00e-05]    [1.69e-02]    \n",
      "10000     [3.72e-05]    [6.83e-05]    [1.81e-02]    \n",
      "\n",
      "Best model at step 9000:\n",
      "  train loss: 3.16e-05\n",
      "  test loss: 6.00e-05\n",
      "  test metric: [1.69e-02]\n",
      "\n",
      "'train' took 45.156488 s\n",
      "\n",
      "Set the default float type to float16\n",
      "Training model...\n",
      "\n",
      "Step      Train loss    Test loss     Test metric   \n",
      "0         [2.57e-01]    [2.19e-01]    [1.02e+00]    \n",
      "1000      [7.72e-03]    [8.78e-03]    [2.05e-01]    \n",
      "2000      [5.42e-03]    [5.58e-03]    [1.64e-01]    \n",
      "3000      [2.84e-03]    [3.40e-03]    [1.28e-01]    \n",
      "4000      [1.31e-03]    [1.61e-03]    [8.79e-02]    \n",
      "5000      [8.90e-04]    [1.25e-03]    [7.75e-02]    \n",
      "6000      [8.76e-04]    [1.24e-03]    [7.70e-02]    \n",
      "7000      [7.93e-04]    [1.18e-03]    [7.52e-02]    \n",
      "8000      [7.31e-04]    [1.13e-03]    [7.34e-02]    \n",
      "9000      [6.97e-04]    [1.11e-03]    [7.30e-02]    \n",
      "10000     [6.82e-04]    [1.08e-03]    [7.19e-02]    \n",
      "\n",
      "Best model at step 10000:\n",
      "  train loss: 6.82e-04\n",
      "  test loss: 1.08e-03\n",
      "  test metric: [7.19e-02]\n",
      "\n",
      "'train' took 48.729024 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "import numpy as np\n",
    "SEED = 0xdde\n",
    "dde.config.set_random_seed(SEED)\n",
    "def f1(x):\n",
    "    return x * np.sin(5 * x)\n",
    "def get_gradients_of_weights(model16):\n",
    "    x_train = model16.data.train_x\n",
    "    y_train = model16.data.train_y\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model16.net.call(x_train)\n",
    "        loss_fn = dde.losses.get(\"MSE\")\n",
    "        loss = loss_fn(y_train, y_pred)\n",
    "    gradients16 = tape.gradient(loss,model16.net.trainable_weights)\n",
    "    gradients161d = np.concatenate([gradient.numpy().ravel() for gradient in gradients16])\n",
    "    return gradients161d\n",
    "def get_weights(model16):\n",
    "    return np.concatenate([weight.flatten() for weight in model16.net.get_weights()])\n",
    "def cos_sim_and_dist_of_vectors(g16, g32):\n",
    "    def cosine_similarity(vector1, vector2):\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        norm_vector1 = np.linalg.norm(vector1)\n",
    "        norm_vector2 = np.linalg.norm(vector2)\n",
    "        return dot_product / (norm_vector1 * norm_vector2)\n",
    "    csim = cosine_similarity(g16, g32)\n",
    "    dist = np.linalg.norm(g16 - g32)\n",
    "\n",
    "    return csim, dist\n",
    "\n",
    "class SaveGradientsCallback(dde.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.list_of_weights = []\n",
    "    def on_epoch_begin(self):\n",
    "        weights = get_weights(self.model)\n",
    "        grads = get_gradients_of_weights(self.model)\n",
    "#         print(np.array(weights).shape, np.array(grads).shape)\n",
    "        self.list_of_weights.append((weights, grads))\n",
    "\n",
    "#\n",
    "# Create the original model as float32\n",
    "#\n",
    "dde.config.set_default_float('float32')\n",
    "\n",
    "geom = dde.geometry.Interval(-1, 1)\n",
    "data = dde.data.Function(geom, f1, 16, 100)\n",
    "\n",
    "net = dde.nn.FNN([1] + [10] * 2 + [1], \"tanh\", \"Glorot uniform\")\n",
    "model32 = dde.Model(data, net)\n",
    "model32.compile(\"adam\", lr=0.001, metrics=[\"l2 relative error\"])\n",
    "model32.train(iterations=0)\n",
    "\n",
    "#\n",
    "# create a new model, but float16\n",
    "#\n",
    "dde.config.set_default_float('float16')\n",
    "\n",
    "geom = dde.geometry.Interval(-1, 1)\n",
    "data = dde.data.Function(geom, f1, 16, 100)\n",
    "\n",
    "net = dde.nn.FNN([1] + [10] * 2 + [1], \"tanh\", \"Glorot uniform\")\n",
    "model16 = dde.Model(data, net)\n",
    "model16.compile(\"adam\", lr=0.001, metrics=[\"l2 relative error\"])\n",
    "model16.train(iterations=0)\n",
    "#\n",
    "# copy the weights over\n",
    "#\n",
    "print(model32.net.__dict__)\n",
    "for i, layer in enumerate(model32.net.denses):\n",
    "    model16.net.denses[i].set_weights(\n",
    "        [tf.cast(w, dtype=tf.float16) for w in layer.get_weights()]\n",
    "    )\n",
    "#\n",
    "# train the models\n",
    "#\n",
    "dde.config.set_default_float('float32')\n",
    "cback32 = SaveGradientsCallback()\n",
    "losshistory, train_state = model32.train(iterations=10_000,\n",
    "                                         callbacks=[cback32]\n",
    ")\n",
    "dde.config.set_default_float('float16')\n",
    "cback16 = SaveGradientsCallback()\n",
    "losshistory, train_state = model16.train(iterations=10_000,callbacks=[cback16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff6ac5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ff6ac5c",
    "outputId": "fe94a192-b053-4a9d-cf97-8af080f41265"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (4147231062.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    ax.plot(epochaxis, grad_mags[:,0], label=\"Float16\", 'r')\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 12\n",
    "#\n",
    "# process the data\n",
    "#\n",
    "cos_similarity_grad = []\n",
    "grad_mags = []\n",
    "dist_grad = []\n",
    "\n",
    "cos_similarity_weights = []\n",
    "mags_weights = []\n",
    "dist_weights = []\n",
    "for (weights16, grads16), (weights32, grads32) in zip(cback16.list_of_weights,cback32.list_of_weights):\n",
    "    # calculate the metrics for the gradients\n",
    "    csim, dist = cos_sim_and_dist_of_vectors(grads16, grads32)\n",
    "    grad_mags.append([np.linalg.norm(grads16), np.linalg.norm(grads32)])\n",
    "    cos_similarity_grad.append(csim)\n",
    "    dist_grad.append(dist)\n",
    "    # calculate the metrics for the weights\n",
    "    csim, dist = cos_sim_and_dist_of_vectors(weights16, weights32)\n",
    "    mags_weights.append([np.linalg.norm(weights16), np.linalg.norm(weights32)])\n",
    "    cos_similarity_weights.append(csim)\n",
    "    dist_weights.append(dist)\n",
    "grad_mags = np.array(grad_mags)\n",
    "mags_weights = np.array(mags_weights)\n",
    "# do some graphing\n",
    "epochaxis = np.linspace(0, 10_000, 10_000)\n",
    "# plot mags_grad (log scale)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, grad_mags[:,0], label=\"float16\")\n",
    "# ax.plot(epochaxis, grad_mags[:,1], label=\"float32\")\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Magnitude of gradients of weights (euclidian norm)')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()\n",
    "# plot mags_grad\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochaxis, grad_mags[:,0],'r', label=\"Float16\")\n",
    "ax.plot(epochaxis, grad_mags[:,1],'b', label=\"Float32\")\n",
    "ax.set_yscale('log')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('$L^2$ norm of gradients')\n",
    "plt.legend(frameon=False)\n",
    "# Set the linewidth of the figure border to 1.5\n",
    "for axis in ['top', 'bottom', 'left', 'right']:\n",
    "    plt.gca().spines[axis].set_linewidth(1.5)\n",
    "\n",
    "\n",
    "plt.savefig(\"mags1632grads.pdf\", format='pdf')\n",
    "plt.show()\n",
    "\n",
    "# # plot cos_similarity_grad\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, cos_similarity_grad, label=\"Cosine similarity between gradients of models\")\n",
    "# ax.set_xscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Cosine similarity between weights vectors')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()\n",
    "\n",
    "# # plot dist_grad\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, dist_grad, label=\"Distance between gradients of weights\")\n",
    "# ax.set_xscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Euclidian distance between gradient vectors')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()\n",
    "\n",
    "### we just did the gradients, now do the weights\n",
    "# print('now the weights')\n",
    "# # plot mags_grad (log scale)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, mags_weights[:,0], label=\"Float16\")\n",
    "# ax.plot(epochaxis, mags_weights[:,1], label=\"Float32\")\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Magnitude of weights (euclidian norm)')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()\n",
    "# # plot mags_grad\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, mags_weights[:,0], label=\"Float16\")\n",
    "# ax.plot(epochaxis, mags_weights[:,1], label=\"Float32\")\n",
    "# ax.set_yscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Magnitude of weights (euclidian norm)')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()\n",
    "\n",
    "# # plot cos_similarity_grad\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, cos_similarity_weights, label=\"Cosine similarity between weights of models\")\n",
    "# ax.set_xscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Cosine similarity between weights vectors')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()\n",
    "\n",
    "# # plot dist_grad\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(epochaxis, dist_weights, label=\"Distance between weights\")\n",
    "# plt.title('Distance between weights')\n",
    "# ax.set_xscale('log')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Euclidian distance between weight vectors')\n",
    "# plt.legend(frameon=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f1d1f-e370-4f79-9009-03ab9284c32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
